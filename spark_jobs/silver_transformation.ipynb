{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43625ca-7467-43d7-86c5-e48f170aa4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8a1e3d-0de5-42b9-b2a5-d14710cd2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"silver_transformation_p3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83f3405-0a9d-4cdd-84d3-a2ba2a537b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICEBERG_WAREHOUSE = os.getenv(\"ICEBERG_WAREHOUSE\", \"s3a://promotionengine-search\")\n",
    "NESSIE_URI = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v2\")\n",
    "NESSIE_REF = os.getenv(\"NESSIE_REF\", \"main\")\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"eu-north-1\")\n",
    "\n",
    "JAR_DIR = os.getenv(\"JAR_DIR\", \"/home/jovyan/jars\")\n",
    "\n",
    "# We only need Iceberg/Nessie/AWS jars for this step (No Mongo needed here, but keeping list safe)\n",
    "JARS = [\n",
    "    os.path.join(JAR_DIR, \"iceberg-spark-runtime-3.4_2.12-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-nessie-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-client-0.99.0.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-spark-extensions-3.4_2.12-0.105.7.jar\"),\n",
    "    os.path.join(JAR_DIR, \"hadoop-aws-3.3.4.jar\"),\n",
    "    os.path.join(JAR_DIR, \"aws-java-sdk-bundle-1.12.772.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongo-spark-connector_2.12-10.1.1.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-core-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-sync-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"bson-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-aws-bundle-1.5.2.jar\"),\n",
    "]\n",
    "\n",
    "# INPUT (BRONZE)\n",
    "SOURCE_TABLE = \"nessie.sales.mongo_orders\"\n",
    "\n",
    "# OUTPUT (SILVER)\n",
    "TARGET_NAMESPACE = \"sales_silver\"\n",
    "TARGET_TABLE = \"mongo_orders_silver\"\n",
    "TARGET_IDENT = f\"nessie.{TARGET_NAMESPACE}.{TARGET_TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce16305c-df83-4e62-8734-286ac5e1d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beec78a2-cbe3-4612-9697-88d07782e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    logger.info(\"Creating SparkSession\")\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"SILVER_TRANSFORMATION_PROJECT3\")\n",
    "        .config(\"spark.jars\", \",\".join(JARS))\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"nessie\")\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "        .config(\"spark.sql.catalog.nessie.ref\", NESSIE_REF)\n",
    "        .config(\"spark.sql.catalog.nessie.warehouse\", ICEBERG_WAREHOUSE)\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87467af0-0b26-44b2-9f5a-700b98b66571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:38:14,939 | INFO | silver_transformation_p3 | ========== JOB STARTED at 2025-12-23 10:38:14 ==========\n",
      "2025-12-23 10:38:14,941 | INFO | silver_transformation_p3 | Creating SparkSession\n",
      "2025-12-23 10:38:25,598 | INFO | silver_transformation_p3 | Reading from BRONZE table: nessie.sales.mongo_orders\n",
      "2025-12-23 10:38:35,774 | INFO | silver_transformation_p3 | Bronze Row Count: 6\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- discount: integer (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: string (nullable = true)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- unit_price: double (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- shipping_address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- landmark: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "2025-12-23 10:38:35,795 | INFO | silver_transformation_p3 | Applying Silver Transformation (Adding Country: INDIA)...\n",
      "2025-12-23 10:38:35,991 | INFO | silver_transformation_p3 | Silver Schema Preview:\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- discount: integer (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: string (nullable = true)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- unit_price: double (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- shipping_address: struct (nullable = false)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |    |-- country: string (nullable = false)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "2025-12-23 10:38:35,993 | INFO | silver_transformation_p3 | Ensuring namespace exists: sales_silver\n",
      "2025-12-23 10:38:36,453 | INFO | silver_transformation_p3 | Writing to Silver Iceberg Table: nessie.sales_silver.mongo_orders_silver\n",
      "2025-12-23 10:38:47,189 | INFO | silver_transformation_p3 | Silver write SUCCESS\n",
      "2025-12-23 10:38:47,189 | INFO | silver_transformation_p3 | Verifying Silver Data...\n",
      "2025-12-23 10:38:50,963 | INFO | silver_transformation_p3 | Sample Record: [Row(shipping_address=Row(city='Hyderabad', state='Telangana', zip='500081', country='INDIA'))]\n",
      "2025-12-23 10:38:50,963 | INFO | silver_transformation_p3 | Stopping Spark\n",
      "2025-12-23 10:38:51,426 | INFO | silver_transformation_p3 | ========== JOB SUCCESS at 2025-12-23 10:38:51 ==========\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger.info(\"========== JOB STARTED at %s ==========\", now())\n",
    "\n",
    "    spark = create_spark()\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Reading from BRONZE table: %s\", SOURCE_TABLE)\n",
    "        df_bronze = spark.read.format(\"iceberg\").load(SOURCE_TABLE)\n",
    "        \n",
    "        logger.info(\"Bronze Row Count: %d\", df_bronze.count())\n",
    "        df_bronze.printSchema()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # TRANSFORMATION LOGIC\n",
    "        # -------------------------------------------------------\n",
    "        logger.info(\"Applying Silver Transformation (Adding Country: INDIA)...\")\n",
    "        \n",
    "        # We rebuild the 'shipping_address' struct adding the new column\n",
    "        df_silver = df_bronze.withColumn(\"shipping_address\", \n",
    "            struct(\n",
    "                col(\"shipping_address.city\"),\n",
    "                col(\"shipping_address.state\"),\n",
    "                col(\"shipping_address.zip\"),\n",
    "                lit(\"INDIA\").alias(\"country\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logger.info(\"Silver Schema Preview:\")\n",
    "        df_silver.printSchema()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # WRITE TO SILVER\n",
    "        # -------------------------------------------------------\n",
    "        logger.info(\"Ensuring namespace exists: %s\", TARGET_NAMESPACE)\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{TARGET_NAMESPACE}\")\n",
    "\n",
    "        logger.info(\"Writing to Silver Iceberg Table: %s\", TARGET_IDENT)\n",
    "        df_silver.writeTo(TARGET_IDENT).createOrReplace()\n",
    "\n",
    "        logger.info(\"Silver write SUCCESS\")\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # VERIFICATION\n",
    "        # -------------------------------------------------------\n",
    "        logger.info(\"Verifying Silver Data...\")\n",
    "        result = spark.sql(f\"SELECT shipping_address FROM {TARGET_IDENT} LIMIT 1\").collect()\n",
    "        logger.info(\"Sample Record: %s\", result)\n",
    "\n",
    "        logger.info(\"Stopping Spark\")\n",
    "        spark.stop()\n",
    "        logger.info(\"========== JOB SUCCESS at %s ==========\", now())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"JOB FAILED: %s\", str(e))\n",
    "        logger.error(\"TRACEBACK:\\n%s\", traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694e7a3-2292-4822-826b-ca3f4f838f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
