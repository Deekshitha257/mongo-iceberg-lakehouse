{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c273687-787c-4b01-a835-56512b0b8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bf55bf-fd10-42e4-9c0e-ea63fa0c6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"gold_reporting_p3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a95237-2b37-4abd-9d3b-5ad8d7929de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessie/Iceberg Config\n",
    "ICEBERG_WAREHOUSE = os.getenv(\"ICEBERG_WAREHOUSE\", \"s3a://promotionengine-search\")\n",
    "NESSIE_URI = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v2\")\n",
    "NESSIE_REF = os.getenv(\"NESSIE_REF\", \"main\")\n",
    "\n",
    "# AWS Config\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"eu-north-1\")\n",
    "\n",
    "# Postgres Config (Destination)\n",
    "# We use the internal container name \"postgres\"\n",
    "PG_URL = \"jdbc:postgresql://postgres:5432/airflow\"\n",
    "PG_USER = \"airflow\"\n",
    "PG_PASS = \"airflow\"\n",
    "PG_TABLE = \"public.city_sales_report\"\n",
    "\n",
    "JAR_DIR = os.getenv(\"JAR_DIR\", \"/home/jovyan/jars\")\n",
    "# ADDED POSTGRES JAR HERE\n",
    "JARS = [\n",
    "    os.path.join(JAR_DIR, \"iceberg-spark-runtime-3.4_2.12-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-nessie-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-client-0.99.0.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-spark-extensions-3.4_2.12-0.105.7.jar\"),\n",
    "    os.path.join(JAR_DIR, \"hadoop-aws-3.3.4.jar\"),\n",
    "    os.path.join(JAR_DIR, \"aws-java-sdk-bundle-1.12.772.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongo-spark-connector_2.12-10.1.1.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-core-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-sync-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"bson-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-aws-bundle-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"postgresql-42.7.3.jar\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# INPUT (SILVER)\n",
    "SOURCE_TABLE = \"nessie.sales_silver.mongo_orders_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad13e2a-8c15-4a49-9ea7-00ad377777da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d0dc11-f52a-4a7b-b301-631256e66d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    logger.info(\"Creating SparkSession\")\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"GOLD_REPORTING_PROJECT3\")\n",
    "        .config(\"spark.jars\", \",\".join(JARS))\n",
    "        \n",
    "        # Iceberg Config\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.defaultCatalog\", \"nessie\")\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "        .config(\"spark.sql.catalog.nessie.ref\", NESSIE_REF)\n",
    "        .config(\"spark.sql.catalog.nessie.warehouse\", ICEBERG_WAREHOUSE)\n",
    "        \n",
    "        # AWS Config\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        \n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32fcf4f-3085-43f6-92db-b7a37e2c65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 11:19:51,484 | INFO | gold_reporting_p3 | ========== JOB STARTED at 2025-12-23 11:19:51 ==========\n",
      "2025-12-23 11:19:51,486 | INFO | gold_reporting_p3 | Creating SparkSession\n",
      "2025-12-23 11:20:06,870 | INFO | gold_reporting_p3 | Reading SILVER table: nessie.sales_silver.mongo_orders_silver\n",
      "2025-12-23 11:20:12,729 | INFO | gold_reporting_p3 | Aggregating Sales by City and Country...\n",
      "2025-12-23 11:20:12,925 | INFO | gold_reporting_p3 | Gold Report Preview:\n",
      "+---------+-------+-------------+-----------+\n",
      "|     city|country|total_revenue|order_count|\n",
      "+---------+-------+-------------+-----------+\n",
      "|Hyderabad|  INDIA|       1525.5|          2|\n",
      "|Bangalore|  INDIA|        440.0|          3|\n",
      "|   Mumbai|  INDIA|        15.99|          1|\n",
      "+---------+-------+-------------+-----------+\n",
      "\n",
      "2025-12-23 11:20:19,321 | INFO | gold_reporting_p3 | Writing to Postgres Table: public.city_sales_report\n",
      "2025-12-23 11:20:22,584 | INFO | gold_reporting_p3 | Postgres write SUCCESS\n",
      "2025-12-23 11:20:22,585 | INFO | gold_reporting_p3 | Stopping Spark\n",
      "2025-12-23 11:20:22,712 | INFO | gold_reporting_p3 | ========== JOB SUCCESS at 2025-12-23 11:20:22 ==========\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger.info(\"========== JOB STARTED at %s ==========\", now())\n",
    "\n",
    "    spark = create_spark()\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Reading SILVER table: %s\", SOURCE_TABLE)\n",
    "        df_silver = spark.read.format(\"iceberg\").load(SOURCE_TABLE)\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # GOLD AGGREGATION LOGIC\n",
    "        # -------------------------------------------------------\n",
    "        logger.info(\"Aggregating Sales by City and Country...\")\n",
    "        \n",
    "        # We access the struct fields using dot notation\n",
    "        df_gold = (\n",
    "            df_silver\n",
    "            .groupBy(\n",
    "                col(\"shipping_address.city\").alias(\"city\"),\n",
    "                col(\"shipping_address.country\").alias(\"country\")\n",
    "            )\n",
    "            .agg(\n",
    "                _sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "                count(\"order_id\").alias(\"order_count\")\n",
    "            )\n",
    "            .orderBy(col(\"total_revenue\").desc())\n",
    "        )\n",
    "\n",
    "        logger.info(\"Gold Report Preview:\")\n",
    "        df_gold.show()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # WRITE TO POSTGRES\n",
    "        # -------------------------------------------------------\n",
    "        logger.info(\"Writing to Postgres Table: %s\", PG_TABLE)\n",
    "        \n",
    "        (\n",
    "            df_gold.write\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", PG_URL)\n",
    "            .option(\"dbtable\", PG_TABLE)\n",
    "            .option(\"user\", PG_USER)\n",
    "            .option(\"password\", PG_PASS)\n",
    "            .option(\"driver\", \"org.postgresql.Driver\")\n",
    "            .mode(\"overwrite\")  # Replaces the table each time\n",
    "            .save()\n",
    "        )\n",
    "\n",
    "        logger.info(\"Postgres write SUCCESS\")\n",
    "\n",
    "        logger.info(\"Stopping Spark\")\n",
    "        spark.stop()\n",
    "        logger.info(\"========== JOB SUCCESS at %s ==========\", now())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"JOB FAILED: %s\", str(e))\n",
    "        logger.error(\"TRACEBACK:\\n%s\", traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fe182-11e3-4f4d-a404-2c55b6b3b9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
