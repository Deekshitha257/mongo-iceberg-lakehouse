{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ab4dbc-ae44-4126-860c-70b2984e9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0685e355-1dc0-4476-b850-cea1276076c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"mongo_to_iceberg_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ecf8db-3c2a-477b-a528-f8494972b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_HOST = os.getenv(\"MONGO_HOST\", \"mongo\")\n",
    "MONGO_URI = (\n",
    "    f\"mongodb://mongo_user:mongo_pass@{MONGO_HOST}:27017/\"\n",
    "    \"airflow_db?authSource=admin\"\n",
    ")\n",
    "\n",
    "ICEBERG_WAREHOUSE = os.getenv(\n",
    "    \"ICEBERG_WAREHOUSE\",\n",
    "    \"s3a://promotionengine-search/warehouse\"\n",
    ")\n",
    "\n",
    "# ⚠️ Iceberg + Spark must use v1\n",
    "NESSIE_URI = os.getenv(\"NESSIE_URI\", \"http://nessie:19120/api/v1\")\n",
    "NESSIE_REF = os.getenv(\"NESSIE_REF\", \"main\")\n",
    "\n",
    "JAR_DIR = os.getenv(\"JAR_DIR\", \"/home/jovyan/jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6674bde-9c5c-497b-8c88-c87fe6c4d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "JARS = [\n",
    "    os.path.join(JAR_DIR, \"iceberg-spark-runtime-3.4_2.12-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-nessie-1.5.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-client-0.99.0.jar\"),\n",
    "    os.path.join(JAR_DIR, \"nessie-spark-extensions-3.4_2.12-0.105.7.jar\"),\n",
    "    os.path.join(JAR_DIR, \"hadoop-aws-3.3.4.jar\"),\n",
    "    os.path.join(JAR_DIR, \"aws-java-sdk-bundle-1.12.772.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongo-spark-connector_2.12-10.1.1.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-core-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"mongodb-driver-sync-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"bson-4.11.2.jar\"),\n",
    "    os.path.join(JAR_DIR, \"iceberg-aws-bundle-1.5.2.jar\"),\n",
    "]\n",
    "\n",
    "NAMESPACE = \"sales\"\n",
    "TABLE = \"mongo_orders\"\n",
    "TABLE_IDENT = f\"nessie.{NAMESPACE}.{TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f57999-c558-4e8e-8e71-858c76cc1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596fe1ca-736b-4cb1-94df-8701e9ed47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jars():\n",
    "    logger.info(\"Checking Spark JARs...\")\n",
    "    missing = []\n",
    "    for jar in JARS:\n",
    "        exists = os.path.exists(jar)\n",
    "        logger.info(\"JAR=%s exists=%s\", jar, exists)\n",
    "        if not exists:\n",
    "            missing.append(jar)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b46b7e54-c071-4cd5-a6c1-fede4bfc92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    logger.info(\"Creating SparkSession\")\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MONGO_TO_ICEBERG_NOTEBOOK\")\n",
    "        .config(\"spark.jars\", \",\".join(JARS))\n",
    "\n",
    "        # Iceberg + Nessie\n",
    "        .config(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "        )\n",
    "        .config(\"spark.sql.defaultCatalog\", \"nessie\")\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.nessie.catalog-impl\",\n",
    "            \"org.apache.iceberg.nessie.NessieCatalog\",\n",
    "        )\n",
    "        .config(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "        .config(\"spark.sql.catalog.nessie.ref\", NESSIE_REF)\n",
    "        .config(\"spark.sql.catalog.nessie.warehouse\", ICEBERG_WAREHOUSE)\n",
    "        .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "        # AWS creds picked automatically from env\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\",\n",
    "        )\n",
    "\n",
    "        # MongoDB\n",
    "        .config(\"spark.mongodb.read.connection.uri\", MONGO_URI)\n",
    "        .config(\"spark.mongodb.write.connection.uri\", MONGO_URI)\n",
    "\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    logger.info(\"Spark version=%s\", spark.version)\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c30c8cd-9a72-46ab-bde9-6ef9c723afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:16:19,791 | INFO | mongo_to_iceberg_nb | ========== JOB STARTED at 2025-12-23 10:16:19 ==========\n",
      "2025-12-23 10:16:19,794 | INFO | mongo_to_iceberg_nb | Checking Spark JARs...\n",
      "2025-12-23 10:16:19,799 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/iceberg-spark-runtime-3.4_2.12-1.5.2.jar exists=True\n",
      "2025-12-23 10:16:19,803 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/iceberg-nessie-1.5.2.jar exists=True\n",
      "2025-12-23 10:16:19,809 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/nessie-client-0.99.0.jar exists=True\n",
      "2025-12-23 10:16:19,814 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/nessie-spark-extensions-3.4_2.12-0.105.7.jar exists=True\n",
      "2025-12-23 10:16:19,818 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/hadoop-aws-3.3.4.jar exists=True\n",
      "2025-12-23 10:16:19,823 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/aws-java-sdk-bundle-1.12.772.jar exists=True\n",
      "2025-12-23 10:16:19,826 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/mongo-spark-connector_2.12-10.1.1.jar exists=True\n",
      "2025-12-23 10:16:19,829 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/mongodb-driver-core-4.11.2.jar exists=True\n",
      "2025-12-23 10:16:19,832 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/mongodb-driver-sync-4.11.2.jar exists=True\n",
      "2025-12-23 10:16:19,835 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/bson-4.11.2.jar exists=True\n",
      "2025-12-23 10:16:19,838 | INFO | mongo_to_iceberg_nb | JAR=/home/jovyan/jars/iceberg-aws-bundle-1.5.2.jar exists=True\n",
      "2025-12-23 10:16:19,840 | INFO | mongo_to_iceberg_nb | Creating SparkSession\n",
      "2025-12-23 10:16:19,858 | INFO | mongo_to_iceberg_nb | Spark version=3.4.1\n",
      "2025-12-23 10:16:19,859 | INFO | mongo_to_iceberg_nb | Reading from MongoDB...\n",
      "2025-12-23 10:16:20,391 | INFO | mongo_to_iceberg_nb | MongoDB row count=6\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- discount: integer (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: string (nullable = true)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- unit_price: double (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- shipping_address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- landmark: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "2025-12-23 10:16:20,396 | INFO | mongo_to_iceberg_nb | Ensuring namespace exists: sales\n",
      "2025-12-23 10:16:25,086 | INFO | mongo_to_iceberg_nb | Writing to Iceberg table: nessie.sales.mongo_orders\n",
      "2025-12-23 10:16:33,221 | INFO | mongo_to_iceberg_nb | Verifying Iceberg read...\n",
      "2025-12-23 10:16:35,047 | INFO | mongo_to_iceberg_nb | Verification COUNT=[Row(cnt=6)]\n",
      "2025-12-23 10:16:35,047 | INFO | mongo_to_iceberg_nb | Stopping Spark\n",
      "2025-12-23 10:16:35,505 | INFO | mongo_to_iceberg_nb | ========== JOB SUCCESS at 2025-12-23 10:16:35 ==========\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger.info(\"========== JOB STARTED at %s ==========\", now())\n",
    "\n",
    "    missing = check_jars()\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing JARs: {missing}\")\n",
    "\n",
    "    spark = create_spark()\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Reading from MongoDB...\")\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"mongodb\")\n",
    "            .option(\"database\", \"airflow_db\")\n",
    "            .option(\"collection\", \"orders\")\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        row_count = df.count()\n",
    "        logger.info(\"MongoDB row count=%d\", row_count)\n",
    "        df.printSchema()\n",
    "\n",
    "        if row_count == 0:\n",
    "            raise RuntimeError(\"MongoDB collection is EMPTY\")\n",
    "\n",
    "        logger.info(\"Ensuring namespace exists: %s\", NAMESPACE)\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS nessie.{NAMESPACE}\")\n",
    "\n",
    "        logger.info(\"Writing to Iceberg table: %s\", TABLE_IDENT)\n",
    "        df.writeTo(TABLE_IDENT).createOrReplace()\n",
    "\n",
    "        logger.info(\"Verifying Iceberg read...\")\n",
    "        result = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {TABLE_IDENT}\").collect()\n",
    "        logger.info(\"Verification COUNT=%s\", result)\n",
    "\n",
    "        logger.info(\"Stopping Spark\")\n",
    "        spark.stop()\n",
    "\n",
    "        logger.info(\"========== JOB SUCCESS at %s ==========\", now())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"JOB FAILED: %s\", str(e))\n",
    "        logger.error(\"TRACEBACK:\\n%s\", traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef7bc3-a359-4b40-8fd8-c756ab460b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
